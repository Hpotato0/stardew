{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErGBb-K_dg0t"
      },
      "source": [
        "# aT 농산품 예측 base line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-XaBftdlts"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pYTIBn6Kdaks"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandasql import sqldf\n",
        "import os\n",
        "\n",
        "\n",
        "# 경고 끄기\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1tQSv1cd57m"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvxJh0SDd5WA"
      },
      "source": [
        "## 전처리 Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m7u5Cj_On8JL"
      },
      "outputs": [],
      "source": [
        "class preprocessing_data(object):\n",
        "\n",
        "    \"\"\"\n",
        "    도매, 소매, 수입수출, 도매경락, 주산지 데이터 전처리용 class\n",
        "    중간결과물 저장 check parameter을 통해 지정, 중간결과물 저장 없이 사용은 check = 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,dir):\n",
        "        \"\"\"\n",
        "        전체 데이터에서 해당하는 domae,imexport,pummok,somae,weather 별 분리\n",
        "        \"\"\"\n",
        "        self.data_list = glob(dir)\n",
        "        self.domae = []\n",
        "        self.imexport = []\n",
        "        self.pummok = []\n",
        "        self.somae = []\n",
        "        self.weather = []\n",
        "\n",
        "\n",
        "        for i in self.data_list:\n",
        "            if 'domae' in i:\n",
        "                self.domae.append(i)\n",
        "            if 'imexport' in i:\n",
        "                self.imexport.append(i)\n",
        "            if 'pummok' in i.split('/')[-1]:\n",
        "                self.pummok.append(i)\n",
        "            if 'somae' in i:\n",
        "                self.somae.append(i)\n",
        "            if 'weather' in i:\n",
        "                self.weather.append(i)\n",
        "\n",
        "\n",
        "    def add_pummock(self,check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        pummock의 데이터를 가져와 '해당일자_전체거래물량', '하위가격 평균가', '상위가격 평균가', '하위가격 거래물량', '상위가격 거래물량' 의 파생변수를 생성하는 단계\n",
        "        \"\"\"\n",
        "\n",
        "        for num in tqdm(self.pummok):\n",
        "            ddf = pd.read_csv(num)  # pummock의 csv 읽어오기\n",
        "            name = num.split('/')[-1] # 전체 정제한 데이터를 담을 변수 이름\n",
        "\n",
        "            sep2 = sqldf(f\"select *, sum(거래량) as '해당일자_전체거래물량(kg)' from ddf group by datadate\")\n",
        "            # sql 문법을 이용해 '해당일자_전체거래물량' 계산\n",
        "\n",
        "            height_set = []\n",
        "            low_set = []\n",
        "            height_volume_set = []\n",
        "            low_volume_set = []\n",
        "\n",
        "            for i in sep2['datadate']:\n",
        "\n",
        "                \"\"\"\n",
        "                sep2는 group by를 통해 각 일자가 합쳐진 상태 예를 들어 '201703' 이 5개 이렇게 있을때 sep2는 group 시켜서 '해당일자_전체거래물량'을 계산\n",
        "                이후 sep2 기준 20170101 and 20220630 사이의 날짜들에 해당하는 각 '201703' 마다 '해당일자_전체평균가격' 보다 큰지 아니면 작은지 판단\n",
        "                위 과정을 통해 '하위가격 평균가', '상위가격 평균가', '하위가격 거래물량', '상위가격 거래물량' 변수 생성\n",
        "                \"\"\"\n",
        "\n",
        "                new_list = ddf.loc[[d for d, x in enumerate(ddf['datadate']) if x == i]]\n",
        "                set_price = sep2.loc[list(sep2['datadate']).index(i)]['해당일자_전체평균가격(원)']\n",
        "\n",
        "                sum_he_as = sum(new_list['거래대금(원)'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z >= set_price)\n",
        "                sum_he_vo = sum(new_list['거래량'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z >= set_price)\n",
        "\n",
        "                sum_lo_as = sum(new_list['거래대금(원)'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z < set_price)\n",
        "                sum_lo_vo = sum(new_list['거래량'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z < set_price)\n",
        "\n",
        "                if sum_lo_vo != 0:\n",
        "                    low_set.append(sum_lo_as / sum_lo_vo)\n",
        "                    low_volume_set.append(sum_lo_vo)\n",
        "                else:\n",
        "                    low_set.append(np.nan)\n",
        "                    low_volume_set.append(np.nan)\n",
        "\n",
        "                if sum_he_vo != 0:\n",
        "                    height_set.append(sum_he_as / sum_he_vo)\n",
        "                    height_volume_set.append(sum_he_vo)\n",
        "                else:\n",
        "                    height_set.append(np.nan)\n",
        "                    height_volume_set.append(np.nan)\n",
        "\n",
        "            sep2['하위가격 평균가(원)'] = low_set\n",
        "            sep2['상위가격 평균가(원)'] = height_set\n",
        "\n",
        "            sep2['하위가격 거래물량(kg)'] = low_volume_set\n",
        "            sep2['상위가격 거래물량(kg)'] = height_volume_set\n",
        "\n",
        "\n",
        "            globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'] = sep2.copy()\n",
        "\n",
        "\n",
        "            # 중간 산출물 저장\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/품목') == False:\n",
        "                    os.mkdir(f'./data/품목')\n",
        "\n",
        "                sep2.to_csv(f'./data/품목/{name}', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def add_dosomae(self, option=1, check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        domae, somae 데이터를 가져와서 정제하는 단계\n",
        "        option parameter을 통한 도매, 소매 선택\n",
        "        \"\"\"\n",
        "\n",
        "        if option == 1:\n",
        "            df = self.domae\n",
        "            text = '도매'\n",
        "        else:\n",
        "            df = self.somae\n",
        "            text = '소매'\n",
        "\n",
        "        for i in tqdm(df):\n",
        "            test = pd.read_csv(i)\n",
        "            name = i.split('/')[-1]\n",
        "\n",
        "            sep = test.loc[(test['등급명'] == '상품') | (test['등급명'] == 'S과')]  # 모든 상품에 대해서 수행하지 않고 GRAD_NM이 '상품', 'S과' 만 해당하는 품목 가져옴\n",
        "            sep = sep[['datadate', '등급명', '조사단위(kg)', '가격(원)']]\n",
        "\n",
        "            sep.rename(columns={\"가격(원)\": \"가격\"}, inplace=True)\n",
        "\n",
        "            sep2 = sqldf(\n",
        "                f\"select datadate, max(가격) as '일자별_{text}가격_최대(원)', avg(가격) as '일자별_{text}가격_평균(원)', min(가격) as '일자별_{text}가격_최소(원)' from sep group by datadate\")\n",
        "\n",
        "            globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'] = globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'].merge(sep2, how='left')\n",
        "\n",
        "            # 중간 산출물 저장\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/{text}') == False:\n",
        "                    os.mkdir(f'./data/{text}')\n",
        "\n",
        "                sep2.to_csv(f'./data/{text}/{name}', index=False)\n",
        "\n",
        "    def add_imexport(self,check=0):\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        imexport 데이터 관련 정제, imexport 데이터는 월별 수입수출 데이터임으로 해당 월에 같은 값을 넣어주고 없는 것에는 np.nan\n",
        "        해당 품목에 대한 imexport 데이터가 없는 경우 np.nan으로 대체, 모든 품목의 데이터가 동일한 컬럼수를 가지기 위해 수행\n",
        "        \"\"\"\n",
        "\n",
        "        imex_cd = [i.split('_')[-1].split('.')[0] for i in self.imexport]\n",
        "\n",
        "        for i in tqdm(range(len(self.pummok))):\n",
        "\n",
        "            cd_number = self.pummok[i].split('_')[-1].split('.')[0]\n",
        "            file_name = 'imexport_' + self.pummok[i].split('pummok_')[1]\n",
        "\n",
        "\n",
        "            if cd_number in imex_cd:\n",
        "                test4 = pd.read_csv(self.imexport[imex_cd.index(cd_number)])\n",
        "\n",
        "                new_exim1 = []\n",
        "                new_exim2 = []\n",
        "                new_exim3 = []\n",
        "                new_exim4 = []\n",
        "                new_exim5 = []\n",
        "\n",
        "                for j in globals()[f'df_{cd_number}']['datadate']:\n",
        "                    target = j//100\n",
        "\n",
        "                    try:\n",
        "                        number = list(test4['datadate']).index(target)\n",
        "\n",
        "\n",
        "                        new_exim1.append(test4['수출중량(kg)'].iloc[number])\n",
        "                        new_exim2.append(test4['수출금액(달러)'].iloc[number])\n",
        "                        new_exim3.append(test4['수입중량(kg)'].iloc[number])\n",
        "                        new_exim4.append(test4['수입금액(달러)'].iloc[number])\n",
        "                        new_exim5.append(test4['무역수지(달러)'].iloc[number])\n",
        "                    except:\n",
        "                        new_exim1.append(np.nan)\n",
        "                        new_exim2.append(np.nan)\n",
        "                        new_exim3.append(np.nan)\n",
        "                        new_exim4.append(np.nan)\n",
        "                        new_exim5.append(np.nan)\n",
        "\n",
        "                df2 = pd.DataFrame()\n",
        "                df2['수출중량(kg)'] = new_exim1\n",
        "                df2['수출금액(달러)'] = new_exim2\n",
        "                df2['수입중량(kg)'] = new_exim3\n",
        "                df2['수입금액(달러)'] = new_exim4\n",
        "                df2['무역수지(달러)'] = new_exim5\n",
        "\n",
        "                globals()[f'df_{cd_number}'] = pd.concat([globals()[f'df_{cd_number}'], df2],axis=1)\n",
        "\n",
        "            else:\n",
        "                df2 = pd.DataFrame()\n",
        "                df2['수출중량(kg)'] = np.nan\n",
        "                df2['수출금액(달러)'] = np.nan\n",
        "                df2['수입중량(kg)'] = np.nan\n",
        "                df2['수입금액(달러)'] = np.nan\n",
        "                df2['무역수지(달러)'] = np.nan\n",
        "\n",
        "                globals()[f'df_{cd_number}'] = pd.concat([globals()[f'df_{cd_number}'], df2], axis=1)\n",
        "\n",
        "\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/수출입') == False:\n",
        "                    os.mkdir(f'./data/수출입')\n",
        "\n",
        "                df2.to_csv(f'./data/수출입/{file_name}', index=False)\n",
        "\n",
        "    def add_weather(self, check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        weather 품목별 주산지 데이터를 가져와 합치는 함수, 일부 품목의 주산지가 3개가 아닌 것에 대해서는 np.nan 값으로 합쳐줌\n",
        "        \"\"\"\n",
        "\n",
        "        for i in tqdm(self.pummok):\n",
        "            name = i.split('_')[-1].split('.')[0]\n",
        "            check_file = [j for j in self.weather if j.split('_')[-2] == name]\n",
        "\n",
        "\n",
        "            df = pd.DataFrame()\n",
        "            for d, j in enumerate(check_file):\n",
        "                weather_df = pd.read_csv(j)\n",
        "                new_exim1, new_exim2, new_exim3, new_exim4, new_exim5, new_exim6 = [], [], [], [], [], []\n",
        "\n",
        "\n",
        "                for k in globals()[f'df_{name}']['datadate']:\n",
        "                    try:\n",
        "                        number = list(weather_df['datadate']).index(k)\n",
        "\n",
        "                        new_exim1.append(weather_df['초기온도(℃)'].iloc[number])\n",
        "                        new_exim2.append(weather_df['최대온도(℃)'].iloc[number])\n",
        "                        new_exim3.append(weather_df['최저온도(℃)'].iloc[number])\n",
        "                        new_exim4.append(weather_df['평균온도(℃)'].iloc[number])\n",
        "                        new_exim5.append(weather_df['강수량(ml)'].iloc[number])\n",
        "                        new_exim6.append(weather_df['습도(%)'].iloc[number])\n",
        "                    except:\n",
        "                        new_exim1.append(np.nan)\n",
        "                        new_exim2.append(np.nan)\n",
        "                        new_exim3.append(np.nan)\n",
        "                        new_exim4.append(np.nan)\n",
        "                        new_exim5.append(np.nan)\n",
        "                        new_exim6.append(np.nan)\n",
        "\n",
        "\n",
        "                df[f'주산지_{d}_초기온도(℃)'] = new_exim1\n",
        "                df[f'주산지_{d}_최대온도(℃)'] = new_exim2\n",
        "                df[f'주산지_{d}_최저온도(℃)'] = new_exim3\n",
        "                df[f'주산지_{d}_평균온도(℃)'] = new_exim4\n",
        "                df[f'주산지_{d}_강수량(ml)'] = new_exim5\n",
        "                df[f'주산지_{d}_습도(%)'] = new_exim6\n",
        "\n",
        "            if len(check_file) < 3:\n",
        "                df[f'주산지_2_초기온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_최대온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_최저온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_평균온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_강수량(ml)'] = np.nan\n",
        "                df[f'주산지_2_습도(%)'] = np.nan\n",
        "\n",
        "            globals()[f'df_{name}'] = pd.concat([globals()[f'df_{name}'], df], axis=1)\n",
        "\n",
        "            if check !=0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/주산지') == False:\n",
        "                    os.mkdir(f'./data/주산지')\n",
        "\n",
        "                df.to_csv(f'./data/주산지/weather_{name}.csv', index=False)\n",
        "\n",
        "    def add_categorical(self, out_dir, data_type=\"train\", check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        일자별 정보를 넣어주는 함수, 월별, 상순, 하순, 중순 을 원핫 인코딩을 통해 데이터로 넣어주는 함수\n",
        "        모델이 각 행마다의 정보에서 몇월인지 상순인지 하순인지 파악하며 훈련시키기 위한 변수\n",
        "        \"\"\"\n",
        "\n",
        "        for i in tqdm(self.pummok):\n",
        "            name = i.split('_')[-1].split('.')[0]\n",
        "\n",
        "            day_set = []\n",
        "            month_set = []\n",
        "\n",
        "            for k in globals()[f'df_{name}']['datadate']:\n",
        "                day = k % 100\n",
        "                month = k % 10000 // 100\n",
        "\n",
        "                if day <= 10:\n",
        "                    day_set.append('초순')\n",
        "                elif (day > 10) and (day <= 20):\n",
        "                    day_set.append('중순')\n",
        "                else:\n",
        "                    day_set.append('하순')\n",
        "\n",
        "                month_set.append(f'{month}월')\n",
        "\n",
        "            globals()[f'df_{name}']['일자구분'] = day_set\n",
        "            globals()[f'df_{name}']['월구분'] = month_set\n",
        "\n",
        "            globals()[f'df_{name}'] = pd.get_dummies(globals()[f'df_{name}'], columns=['일자구분', '월구분'])\n",
        "\n",
        "            if check !=0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if data_type != \"train\":\n",
        "                    if os.path.exists(f'./data/{data_type}') == False:\n",
        "                        os.mkdir(f\"./data/{data_type}\")\n",
        "                    if os.path.exists(f'./data/{data_type}/{out_dir}') == False:\n",
        "                        os.mkdir(f'./data/{data_type}/{out_dir}')\n",
        "                    globals()[f'df_{name}'].to_csv(f'./data/{data_type}/{out_dir}/{data_type}_{name}.csv', index=False)\n",
        "                else:\n",
        "                    if os.path.exists(f'./data/{out_dir}') == False:\n",
        "                        os.mkdir(f'./data/{out_dir}')\n",
        "                    globals()[f'df_{name}'].to_csv(f'./data/{out_dir}/{data_type}_{name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfvySP5EeHJY"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wPpioa1eMX2"
      },
      "source": [
        "## 훈련 데이터 전처리 및 저장 (중간저장 X, 최종저장 O) - train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnvCAIT4WxDQ",
        "outputId": "0afecc06-19f0-4d38-86a7-33c08c9ffa87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "data = preprocessing_data('./aT_train_raw/*.csv')\n",
        "data.add_pummock()\n",
        "data.add_dosomae()\n",
        "data.add_dosomae(option=2)\n",
        "data.add_imexport()\n",
        "data.add_weather()\n",
        "data.add_categorical('train', data_type=\"train\" ,check=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9wDIksvErH"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhBfk96yuxPH"
      },
      "source": [
        "## 검증 데이터셋 전처리 및 저장 (중간저장 X, 최종저장 O) - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZtIuAOFh2x2",
        "outputId": "24e580be-3fc3-4933-9935-78cd84765b3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    data = preprocessing_data(f'./aT_test_raw/sep_{i}/*.csv')\n",
        "    data.add_pummock()\n",
        "    data.add_dosomae()\n",
        "    data.add_dosomae(option=2)\n",
        "    data.add_imexport()\n",
        "    data.add_weather()\n",
        "    data.add_categorical(f'set_{i}', data_type=\"test\", check=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWsIlzqevYwJ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-OTLfDXvbvX"
      },
      "source": [
        "## 입력 shape 및 형태 정의 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2lmKfDzfu9Tp"
      },
      "outputs": [],
      "source": [
        "def make_Tensor(array):\n",
        "    return tf.convert_to_tensor(array, dtype=tf.float32)\n",
        "\n",
        "def astype_data(data):\n",
        "    df = data.astype(np.float32)\n",
        "    return make_Tensor(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0UYIIbavf3F"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzSS2gBcvg-A"
      },
      "source": [
        "## Transformer 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_hTmh7IvljV"
      },
      "source": [
        "- encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BsjSitfdvfNa"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8UiEifWvoO4"
      },
      "source": [
        "- build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BZRwGsc3vnwI"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(28)(x) # 4주 예측\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OEE7TQsvvrW"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPEtzTrJvy84"
      },
      "source": [
        "## keras eraly stop, chekpoint 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tahe2wY8vr83"
      },
      "outputs": [],
      "source": [
        "def call_back_set(name, epoch, batch_size):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "    if os.path.exists(f'./check') == False:\n",
        "        os.mkdir(f'./check')\n",
        "\n",
        "    filename = f'./check/{name}-{epoch}-{batch_size}.h5'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filename,\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 save_weights_only=True,\n",
        "                                 mode='auto'\n",
        "                                 )\n",
        "    return [early_stopping, checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiH83x8Ov4Hy"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE5xZZwbv5PP"
      },
      "source": [
        "## Model 훈련 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7JQWwQQrv3qc"
      },
      "outputs": [],
      "source": [
        "def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate = 0.001, verbose = 1):\n",
        "\n",
        "\n",
        "    model = build_model(\n",
        "    x_train.shape[1:],\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mean_squared_error\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history1 = model.fit(\n",
        "            x_train, y_train,\n",
        "            epochs = epoch,\n",
        "            steps_per_epoch=len(x_train) / batch_size,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_val, y_val),\n",
        "            validation_steps=len(x_val) / batch_size,\n",
        "            shuffle=False,\n",
        "            callbacks=call_back_set(name, epoch, batch_size),\n",
        "            verbose=verbose)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZoGpxWwAWM"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BemGVdjwBw5"
      },
      "source": [
        "## 시점 윈도우 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pDD_c3buv99w"
      },
      "outputs": [],
      "source": [
        "def time_window(df, t, t_sep):\n",
        "    seq_len = t\n",
        "    seqence_length = seq_len + t_sep\n",
        "\n",
        "    result = []\n",
        "    for index in tqdm(range(len(df) - seqence_length)):\n",
        "        result.append(df[index: index + seqence_length].values)\n",
        "\n",
        "    return np.array(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmM1U849wEab"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8rP8wMiwMm7"
      },
      "source": [
        "## 데이터 불러오기 및 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WPx288BJwDmv"
      },
      "outputs": [],
      "source": [
        "data_list = glob('./data//train/*.csv')\n",
        "epoch = 1000\n",
        "batch = 15\n",
        "tr_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 '] # train 에서 사용하지 않는 열\n",
        "ts_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체평균가격(원)'] # test 에서 사용하지 않는 열\n",
        "check_col = ['일자구분_중순', '일자구분_초순', '일자구분_하순','월구분_10월', '월구분_11월', '월구분_12월', '월구분_1월', '월구분_2월', '월구분_3월', \n",
        "             '월구분_4월','월구분_5월', '월구분_6월', '월구분_7월', '월구분_8월', '월구분_9월'] # 열 개수 맞추기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ayr_sBwXf5"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV90ogq-wZlC"
      },
      "source": [
        "## Train 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIMdTeflwQsJ",
        "outputId": "f61b71fd-97a0-4026-99a7-cc24e73f9a19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(data_list):\n",
        "    df_number = i.split(\"_\")[-1].split(\".\")[0]\n",
        "    df = pd.read_csv(i)\n",
        "\n",
        "    for j in df.columns:\n",
        "        df[j] = df[j].replace({' ': np.nan})\n",
        "\n",
        "    # 사용할 열 선택 및 index 설정\n",
        "    df.drop(tr_del_list, axis=1, inplace=True)\n",
        "    df.set_index('datadate', drop=True, inplace=True)\n",
        "\n",
        "    # nan 처리\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # 변수와 타겟 분리\n",
        "    x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격(원)']], df['해당일자_전체평균가격(원)']\n",
        "\n",
        "    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n",
        "    y = y[14:]\n",
        "\n",
        "    # time series window 생성\n",
        "    data_x = time_window(x, 13, 1)\n",
        "    data_y = time_window(y, 27, 1)\n",
        "\n",
        "    # y의 길이와 같은 길이로 설정\n",
        "    xdata = data_x[:len(data_y)]\n",
        "    ydata = data_y\n",
        "\n",
        "    # train, validation 분리 (8 : 2)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.2, shuffle=False, random_state=119)\n",
        "\n",
        "    # transformer 모델 훈련\n",
        "    transformer = train(astype_data(x_train), y_train, astype_data(x_val), y_val, f'transformer-{df_number}', epoch,\n",
        "                        batch)\n",
        "    transformer.load_weights(f'./check/transformer-{df_number}-{epoch}-{batch}.h5')\n",
        "\n",
        "    if os.path.exists(f'./model') == False:\n",
        "        os.mkdir(f'./model')\n",
        "\n",
        "    # 모델 저장\n",
        "    transformer.save(f'./model/transformer-{df_number}-{epoch}-{batch}.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuUtQSv6wm7N"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXzEPY6_woE6"
      },
      "source": [
        "## Test 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zec8tU0ywlG0",
        "outputId": "74350633-2ea7-4cf2-aff8-7e5b641ce1e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 6435.94it/s]\n"
          ]
        }
      ],
      "source": [
        "zero_csv = [0 for i in range(14)]  # 시점이 비어있는 데이터 0으로 채우기 위한 변수\n",
        "\n",
        "for i in tqdm(range(10)):\n",
        "    data_list = glob(f'./data/test/set_{i}/*.csv')\n",
        "\n",
        "    for idx,j in enumerate(data_list):\n",
        "        df = pd.read_csv(j)\n",
        "\n",
        "        if len(df) == 0:\n",
        "            df['zero_non'] = zero_csv\n",
        "            df = df.fillna(0)\n",
        "            df.drop('zero_non', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "        file_number = j.split('test_')[1].split('.')[0]\n",
        "\n",
        "        # 사용할 열 선택, index 설정\n",
        "        df.drop(ts_del_list, axis=1, inplace=True)\n",
        "        df.set_index('datadate', drop=True, inplace=True)\n",
        "\n",
        "        # train input 과 형상 맞추기\n",
        "        add_col = [i for i in check_col if i not in df.columns]\n",
        "\n",
        "        for a in add_col:\n",
        "            df[a] = 0\n",
        "\n",
        "        # ' ' -> nan 으로 변경\n",
        "        for a in df.columns:\n",
        "            df[a] = df[a].replace({' ': np.nan})\n",
        "\n",
        "        # nan 처리\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        # x_test  생성\n",
        "        df_test = astype_data(df.values.reshape(1, df.values.shape[0], df.values.shape[1]))\n",
        "\n",
        "\n",
        "        # model test\n",
        "        if os.path.exists('./model_output') == False:\n",
        "            os.mkdir('./model_output')\n",
        "\n",
        "        if os.path.exists(f'./model_output/set_{i}') == False:\n",
        "            os.mkdir(f'./model_output/set_{i}')\n",
        "\n",
        "        # 해당하는 모델 불러오기\n",
        "        model_test = tf.keras.models.load_model(f'./model/transformer-{file_number}-{epoch}-{batch}.h5')\n",
        "        pred = transformer.predict(df_test)\n",
        "\n",
        "\n",
        "        # 결과 저장\n",
        "        save_df = pd.DataFrame(pred).T\n",
        "        save_df.to_csv(f'./model_output/set_{i}/predict_{file_number}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaiVnMjZVGIQ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBzRP7HJVHaP"
      },
      "source": [
        "## 정답 제출 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnvUoVirpYL",
        "outputId": "3efd024e-5a8d-4973-d797-025b03626876"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['품목0', '품목1', '품목2', '품목3', '품목4', '품목5', '품목6', '품목7', '품목8', '품목9',\\n       '품목10', '품목11', '품목12', '품목13', '품목14', '품목15', '품목16', '품목17', '품목18',\\n       '품목19', '품목20', '품목21', '품목22', '품목23', '품목24', '품목25', '품목26', '품목27',\\n       '품목28', '품목29', '품목30', '품목31', '품목32', '품목33', '품목34', '품목35', '품목36'],\\n      dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/ejchung/HOME/aT_data/baseline.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ejchung/HOME/aT_data/baseline.ipynb#Y100sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         base_number \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ejchung/HOME/aT_data/baseline.ipynb#Y100sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39mglobals\u001b[39m()[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mset_df_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m품목\u001b[39m\u001b[39m{\u001b[39;00mnumber\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m]  \u001b[39m=\u001b[39m [base_number] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(df[df\u001b[39m.\u001b[39mcolumns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mvalues) \u001b[39m# 각 품목당 순서를 t, t+1 ... t+28 로 변경\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ejchung/HOME/aT_data/baseline.ipynb#Y100sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mglobals\u001b[39m()[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mset_df_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mglobals\u001b[39;49m()[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mset_df_\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m][[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m품목\u001b[39;49m\u001b[39m{\u001b[39;49;00mcol\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39mfor\u001b[39;49;00m col \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m37\u001b[39;49m)]]\n",
            "File \u001b[0;32m~/miniforge3/envs/nongsan/lib/python3.8/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
            "File \u001b[0;32m~/miniforge3/envs/nongsan/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/nongsan/lib/python3.8/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['품목0', '품목1', '품목2', '품목3', '품목4', '품목5', '품목6', '품목7', '품목8', '품목9',\\n       '품목10', '품목11', '품목12', '품목13', '품목14', '품목15', '품목16', '품목17', '품목18',\\n       '품목19', '품목20', '품목21', '품목22', '품목23', '품목24', '품목25', '품목26', '품목27',\\n       '품목28', '품목29', '품목30', '품목31', '품목32', '품목33', '품목34', '품목35', '품목36'],\\n      dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "for k in tqdm(range(10)):\n",
        "\n",
        "  globals()[f'set_df_{k}'] = pd.DataFrame()\n",
        "  answer_df_list = glob(f'./model_output/set_{k}/*.csv') # 예측한 결과 불러오기\n",
        "  pum_list = glob(f'./aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n",
        "  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n",
        "\n",
        "  for i in answer_df_list:\n",
        "    df = pd.read_csv(i)\n",
        "    number = i.split('_')[-1].split('.')[0]\n",
        "\n",
        "    base_number = 0\n",
        "    for p in pummok:\n",
        "      if number == p.split('_')[-1].split('.')[0]:\n",
        "        pum_df = pd.read_csv(p)\n",
        "\n",
        "        if len(pum_df) != 0:\n",
        "           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n",
        "        else:\n",
        "          base_number = np.nan\n",
        "\n",
        "    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n",
        "\n",
        "  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snOtIhAGaCZ"
      },
      "source": [
        "- 변동률 계산을 위한 t, t+1 ... t+28 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "4KtiJup_xl00",
        "outputId": "b1d49465-0015-43c5-c849-52eccd434d2c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "set_df_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaTBCR5y3DA5"
      },
      "source": [
        "- 변동률 계산 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUAw88bXupw7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n",
        "\n",
        "\n",
        "for k in range(10):\n",
        "  globals()[f'answer_df_{k}'] = pd.DataFrame()\n",
        "  for c in globals()[f'set_df_{k}'].columns:\n",
        "    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n",
        "\n",
        "    ans_1_14 = []\n",
        "    for i in range(14):\n",
        "      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1]- base_d)/base_d)  # t+1 ~ t+14 까지는 (t+n - t)/t 로 계산\n",
        "\n",
        "    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n",
        "\n",
        "    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n",
        "  \n",
        "  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n",
        "  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXfNkur3Hl4"
      },
      "source": [
        "- sep 0  ~ sep 9 까지 합치기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrEcol7zz0xY"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# 위에서 계산된 변동률 들을 합쳐주는 과정\n",
        "\n",
        "all_df =pd.DataFrame()\n",
        "for i in range(10):\n",
        "  if i== 0 :\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n",
        "  else:\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n",
        "\n",
        "\n",
        "all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n",
        "all_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRvcBaMD82b"
      },
      "source": [
        "- 정답 양식으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDCkhEIz10g1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# set, 일자 기억하기위해 따로 저장\n",
        "\n",
        "re_set = list(all_df['Set'])\n",
        "re_date = list(all_df['일자'])\n",
        "\n",
        "\n",
        "# 정답 양식 불러오기\n",
        "out_ans = pd.read_csv('./answer_example.csv')\n",
        "\n",
        "# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n",
        "submit_df = all_df + out_ans\n",
        "\n",
        "submit_df['Set'] = re_set\n",
        "submit_df['일자'] = re_date\n",
        "\n",
        "\n",
        "# 최종 저장\n",
        "submit_df.to_csv('./submit.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDA0Hq3jFv39"
      },
      "source": [
        "- 계산된 변동률 결과물"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "_mKkxdrSFzpm",
        "outputId": "ca1021be-53b0-436d-8d19-bdb45ffb5306"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ZIqhpdF26-"
      },
      "source": [
        "- 제출 양식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lgBJdo0iF45J",
        "outputId": "3dfb25b5-ac39-47a9-da29-7871eebb98c2"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "out_ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQbCc_EJFuJL"
      },
      "source": [
        "- 제출 양식 반영한 최종 결과물 (**실 제출용**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Yd7vYCmaERDh",
        "outputId": "6f235726-70ff-4ed8-d0df-bf8b44d4eacf"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.8.13 ('nongsan')' requires jupyter and notebook package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "submit_df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "aT_베이스라인.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nongsan')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccf34f8d528451f3e50f16fb623fcecc5366e303508c870c81064262b2c7e9f5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
